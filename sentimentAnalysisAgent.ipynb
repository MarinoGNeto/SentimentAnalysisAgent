{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "czu99vSpBm1y",
    "outputId": "adf4d702-86a4-447d-b24e-002ebcba8295"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Dicionário de abreviações\n",
    "abbreviations = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    # Adicione mais conforme necessário\n",
    "}\n",
    "\n",
    "# Função para substituir emojis por palavras\n",
    "def replace_emoji_with_words(text):\n",
    "    return emoji.demojize(text)  # Converte emojis para a representação textual como :smile:\n",
    "\n",
    "# Função para expandir abreviações\n",
    "def expand_abbreviations(text):\n",
    "    return \" \".join([abbreviations.get(word, word) for word in text.split()])\n",
    "\n",
    "# Função de pré-processamento\n",
    "def preprocess(textdata):\n",
    "    \"\"\"\n",
    "    Ajustes e normalização dos textos\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_text = []\n",
    "\n",
    "    for text in textdata:\n",
    "        # Transformações textuais\n",
    "        text = text.lower()\n",
    "        text = replace_emoji_with_words(text)  # Substituir emojis por palavras\n",
    "        text = expand_abbreviations(text)  # Expandir abreviações\n",
    "        text = re.sub(r\"((http|https|www)[^ ]*)\", \"URL\", text)  # Substituir URLs\n",
    "        text = re.sub(r\"@[^\\s]+\", \"USER\", text)  # Substituir menções\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)  # Remover pontuação\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalizar espaços\n",
    "\n",
    "        # Lematização\n",
    "        tweet_words = [lemmatizer.lemmatize(word) for word in text.split() if len(word) > 1]\n",
    "        processed_text.append(\" \".join(tweet_words))\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Função: Carregar o dataset\n",
    "def load_dataset(file_path, sample_size=150000):\n",
    "    columns = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    dataset = pd.read_csv(file_path, encoding=\"ISO-8859-1\", names=columns, on_bad_lines=\"skip\")\n",
    "    dataset = dataset[[\"sentiment\", \"text\"]].replace(4, 1)  # Corrige a classe de sentimentos\n",
    "\n",
    "    # Balanceando o dataset\n",
    "    negative_data = dataset[dataset[\"sentiment\"] == 0].sample(sample_size, random_state=42)\n",
    "    positive_data = dataset[dataset[\"sentiment\"] == 1].sample(sample_size, random_state=42)\n",
    "\n",
    "    balanced_dataset = pd.concat([negative_data, positive_data]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Conjunto de dados carregado e balanceado: {len(balanced_dataset)} amostras (50% positivos, 50% negativos).\")\n",
    "    return balanced_dataset\n",
    "\n",
    "# Função: Treinamento e avaliação dos modelos\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Treinando múltiplos modelos e avaliando suas performances.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
    "        \"BernoulliNB\": BernoulliNB(alpha=2),\n",
    "        \"LinearSVC\": LinearSVC(),\n",
    "        \"LogisticRegression\": LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTreinando modelo {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Avaliação para {name}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        results[name] = model\n",
    "\n",
    "    return results\n",
    "\n",
    "# Função: Plotar matriz de confusão\n",
    "def plot_confusion_matrix(model, X_test, y_test):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories = [\"Negativos\", \"Positivos\"]\n",
    "    group_names = [\"Verdadeiro Negativo\", \"Falso Positivo\", \"Falso Negativo\", \"Verdadeiro Positivo\"]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot=labels, cmap=\"Blues\", fmt=\"\", xticklabels=categories, yticklabels=categories)\n",
    "    plt.title(\"Matriz de Confusão\")\n",
    "    plt.xlabel(\"Valores previstos\")\n",
    "    plt.ylabel(\"Valores Reais\")\n",
    "    plt.show()\n",
    "\n",
    "# Código Principal/Base\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregando e realizando pré-processamento\n",
    "    dataset = load_dataset(\"tweetsDatasetReduced.csv\")\n",
    "    text, sentiment = dataset[\"text\"], dataset[\"sentiment\"]\n",
    "\n",
    "    print(\"Realizando pré-processamento do texto...\")\n",
    "    start_time = time.time()\n",
    "    processed_text = preprocess(text)\n",
    "    print(f\"Pré-processamento finalizado em {time.time() - start_time:.2f} segundos.\")\n",
    "\n",
    "    # Divisão de dados\n",
    "    X_train, X_test, y_train, y_test = train_test_split(processed_text, sentiment, test_size=0.3, random_state=42)\n",
    "    print(f\"Dados divididos: {len(X_train)} amostras de treinamento e {len(X_test)} amostras de teste.\")\n",
    "\n",
    "    # Vetorização\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=500000)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    print(\"Dados vetorizados.\")\n",
    "\n",
    "    # Treinando e avaliando os modelos\n",
    "    models = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Matriz de confusão\n",
    "    plot_confusion_matrix(models[\"LogisticRegression\"], X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
